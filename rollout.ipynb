{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Attention Rollout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** In this notebook we explore the implementation of **Attention Rollout** on ViT model. The main goals are to understand how the attention layers were extracted and how they implemented the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vit_rollout import VITAttentionRollout\n",
    "from vit_grad_rollout import VITAttentionGradRollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head): Linear(in_features=192, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', \n",
    "        'deit_tiny_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Attention Rollout function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def rollout(attentions, discard_ratio, head_fusion):\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=1)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=1)[0]\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=1)[0]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "\n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    mask = result[0, 0 , 1 :]\n",
    "    # In case of 224x224 image, this brings us from 196 to 14\n",
    "    width = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(width, width).numpy()\n",
    "    mask = mask / np.max(mask)\n",
    "    return mask    \n",
    "\n",
    "class VITAttentionRollout:\n",
    "    def __init__(self, model, attention_layer_name='attn_drop', head_fusion=\"mean\",\n",
    "        discard_ratio=0.9):\n",
    "        self.model = model\n",
    "        self.head_fusion = head_fusion\n",
    "        self.discard_ratio = discard_ratio\n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "        self.attentions = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        self.attentions = []\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        return rollout(self.attentions, self.discard_ratio, self.head_fusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def rollout(attentions, discard_ratio, head_fusion):\n",
    "    result = torch.eye(attentions[0].size(-1)) # Here it seems it starts the reccurent relation\n",
    "    # Since the first matrix is only taken as it is, it starts with identity (see *1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for attention in attentions: # iterates through all attention layers\n",
    "    \n",
    "            '''\n",
    "            The attention tensor has shape 1 x 3 x 197 x 197\n",
    "            Hence in this first part we are either taking the mean, max or mean \n",
    "            across all channels \n",
    "            '''\n",
    "            \n",
    "            # Selects the type of diffusion being used\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=1)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=1)[0] # [0] takes the max value, whereas [1] takes the indice where it was max\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=1)[0]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "            \n",
    "            '''\n",
    "            Hence now we only have 1 channel, each tensor of dimension 1 x 197 x 197\n",
    "            '''\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1) # flattens 1 x 197**2\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False) # The False is for largest \n",
    "            '''\n",
    "            torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\n",
    "                Returns the k largest elements of the given input tensor along a given dimension.\n",
    "                \n",
    "                Most likely drops to speed up computations?\n",
    "            '''\n",
    "            indices = indices[indices != 0] ## Here it ensures that the class token is never droped\n",
    "            flat[0, indices] = 0 ## Set the lowest to 0\n",
    "            # It flattens to make changes inplace easier, below we go back to original tensor 1 x 197 x 197\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2 ## Apparently here we add the residual and normalize\n",
    "            a = a / a.sum(dim=-1) # Normalization, Chapter 3 and appendix A.1 -> NOT CLEAR \n",
    "            # Almost sure there is a typo, should be a.sum(dim=-1, keepdim=True) as in the blog row should be normalized\n",
    "\n",
    "            result = torch.matmul(a, result) # (*1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    mask = result[0, 0 , 1 :] # [0: batch (1 image), 0: attentions of class token, '1 :': takes out the class token]\n",
    "    # hence we end up with a mask of shape 196 (since we had 196 small squares (image split in 14 x 14)) (case 224x224)\n",
    "    \n",
    "    width = int(mask.size(-1)**0.5) # gets square root of 196 which is 14\n",
    "    mask = mask.reshape(width, width).numpy() # put back in image format -> 14 x 14\n",
    "    mask = mask / np.max(mask) # normalize for plotting\n",
    "    return mask # return back -> we remark that it is in only one single channel\n",
    "\n",
    "class VITAttentionRollout:\n",
    "    def __init__(self, model, attention_layer_name='attn_drop', head_fusion=\"mean\",\n",
    "        discard_ratio=0.9):\n",
    "        self.model = model\n",
    "        self.head_fusion = head_fusion\n",
    "        self.discard_ratio = discard_ratio \n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "        self.attentions = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        ## It is only getting the output of the module.... so it seems it is a function of the image we are looking at.\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        self.attentions = []\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        return rollout(self.attentions, self.discard_ratio, self.head_fusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how it extracts the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patch_embed\n",
      "patch_embed.proj\n",
      "patch_embed.norm\n",
      "pos_drop\n",
      "norm_pre\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.norm1\n",
      "blocks.0.attn\n",
      "blocks.0.attn.qkv\n",
      "blocks.0.attn.attn_drop\n",
      "blocks.0.attn.proj\n",
      "blocks.0.attn.proj_drop\n",
      "blocks.0.ls1\n",
      "blocks.0.drop_path1\n",
      "blocks.0.norm2\n",
      "blocks.0.mlp\n",
      "blocks.0.mlp.fc1\n",
      "blocks.0.mlp.act\n",
      "blocks.0.mlp.drop1\n",
      "blocks.0.mlp.fc2\n",
      "blocks.0.mlp.drop2\n",
      "blocks.0.ls2\n",
      "blocks.0.drop_path2\n",
      "blocks.1\n",
      "blocks.1.norm1\n",
      "blocks.1.attn\n",
      "blocks.1.attn.qkv\n",
      "blocks.1.attn.attn_drop\n",
      "blocks.1.attn.proj\n",
      "blocks.1.attn.proj_drop\n",
      "blocks.1.ls1\n",
      "blocks.1.drop_path1\n",
      "blocks.1.norm2\n",
      "blocks.1.mlp\n",
      "blocks.1.mlp.fc1\n",
      "blocks.1.mlp.act\n",
      "blocks.1.mlp.drop1\n",
      "blocks.1.mlp.fc2\n",
      "blocks.1.mlp.drop2\n",
      "blocks.1.ls2\n",
      "blocks.1.drop_path2\n",
      "blocks.2\n",
      "blocks.2.norm1\n",
      "blocks.2.attn\n",
      "blocks.2.attn.qkv\n",
      "blocks.2.attn.attn_drop\n",
      "blocks.2.attn.proj\n",
      "blocks.2.attn.proj_drop\n",
      "blocks.2.ls1\n",
      "blocks.2.drop_path1\n",
      "blocks.2.norm2\n",
      "blocks.2.mlp\n",
      "blocks.2.mlp.fc1\n",
      "blocks.2.mlp.act\n",
      "blocks.2.mlp.drop1\n",
      "blocks.2.mlp.fc2\n",
      "blocks.2.mlp.drop2\n",
      "blocks.2.ls2\n",
      "blocks.2.drop_path2\n",
      "blocks.3\n",
      "blocks.3.norm1\n",
      "blocks.3.attn\n",
      "blocks.3.attn.qkv\n",
      "blocks.3.attn.attn_drop\n",
      "blocks.3.attn.proj\n",
      "blocks.3.attn.proj_drop\n",
      "blocks.3.ls1\n",
      "blocks.3.drop_path1\n",
      "blocks.3.norm2\n",
      "blocks.3.mlp\n",
      "blocks.3.mlp.fc1\n",
      "blocks.3.mlp.act\n",
      "blocks.3.mlp.drop1\n",
      "blocks.3.mlp.fc2\n",
      "blocks.3.mlp.drop2\n",
      "blocks.3.ls2\n",
      "blocks.3.drop_path2\n",
      "blocks.4\n",
      "blocks.4.norm1\n",
      "blocks.4.attn\n",
      "blocks.4.attn.qkv\n",
      "blocks.4.attn.attn_drop\n",
      "blocks.4.attn.proj\n",
      "blocks.4.attn.proj_drop\n",
      "blocks.4.ls1\n",
      "blocks.4.drop_path1\n",
      "blocks.4.norm2\n",
      "blocks.4.mlp\n",
      "blocks.4.mlp.fc1\n",
      "blocks.4.mlp.act\n",
      "blocks.4.mlp.drop1\n",
      "blocks.4.mlp.fc2\n",
      "blocks.4.mlp.drop2\n",
      "blocks.4.ls2\n",
      "blocks.4.drop_path2\n",
      "blocks.5\n",
      "blocks.5.norm1\n",
      "blocks.5.attn\n",
      "blocks.5.attn.qkv\n",
      "blocks.5.attn.attn_drop\n",
      "blocks.5.attn.proj\n",
      "blocks.5.attn.proj_drop\n",
      "blocks.5.ls1\n",
      "blocks.5.drop_path1\n",
      "blocks.5.norm2\n",
      "blocks.5.mlp\n",
      "blocks.5.mlp.fc1\n",
      "blocks.5.mlp.act\n",
      "blocks.5.mlp.drop1\n",
      "blocks.5.mlp.fc2\n",
      "blocks.5.mlp.drop2\n",
      "blocks.5.ls2\n",
      "blocks.5.drop_path2\n",
      "blocks.6\n",
      "blocks.6.norm1\n",
      "blocks.6.attn\n",
      "blocks.6.attn.qkv\n",
      "blocks.6.attn.attn_drop\n",
      "blocks.6.attn.proj\n",
      "blocks.6.attn.proj_drop\n",
      "blocks.6.ls1\n",
      "blocks.6.drop_path1\n",
      "blocks.6.norm2\n",
      "blocks.6.mlp\n",
      "blocks.6.mlp.fc1\n",
      "blocks.6.mlp.act\n",
      "blocks.6.mlp.drop1\n",
      "blocks.6.mlp.fc2\n",
      "blocks.6.mlp.drop2\n",
      "blocks.6.ls2\n",
      "blocks.6.drop_path2\n",
      "blocks.7\n",
      "blocks.7.norm1\n",
      "blocks.7.attn\n",
      "blocks.7.attn.qkv\n",
      "blocks.7.attn.attn_drop\n",
      "blocks.7.attn.proj\n",
      "blocks.7.attn.proj_drop\n",
      "blocks.7.ls1\n",
      "blocks.7.drop_path1\n",
      "blocks.7.norm2\n",
      "blocks.7.mlp\n",
      "blocks.7.mlp.fc1\n",
      "blocks.7.mlp.act\n",
      "blocks.7.mlp.drop1\n",
      "blocks.7.mlp.fc2\n",
      "blocks.7.mlp.drop2\n",
      "blocks.7.ls2\n",
      "blocks.7.drop_path2\n",
      "blocks.8\n",
      "blocks.8.norm1\n",
      "blocks.8.attn\n",
      "blocks.8.attn.qkv\n",
      "blocks.8.attn.attn_drop\n",
      "blocks.8.attn.proj\n",
      "blocks.8.attn.proj_drop\n",
      "blocks.8.ls1\n",
      "blocks.8.drop_path1\n",
      "blocks.8.norm2\n",
      "blocks.8.mlp\n",
      "blocks.8.mlp.fc1\n",
      "blocks.8.mlp.act\n",
      "blocks.8.mlp.drop1\n",
      "blocks.8.mlp.fc2\n",
      "blocks.8.mlp.drop2\n",
      "blocks.8.ls2\n",
      "blocks.8.drop_path2\n",
      "blocks.9\n",
      "blocks.9.norm1\n",
      "blocks.9.attn\n",
      "blocks.9.attn.qkv\n",
      "blocks.9.attn.attn_drop\n",
      "blocks.9.attn.proj\n",
      "blocks.9.attn.proj_drop\n",
      "blocks.9.ls1\n",
      "blocks.9.drop_path1\n",
      "blocks.9.norm2\n",
      "blocks.9.mlp\n",
      "blocks.9.mlp.fc1\n",
      "blocks.9.mlp.act\n",
      "blocks.9.mlp.drop1\n",
      "blocks.9.mlp.fc2\n",
      "blocks.9.mlp.drop2\n",
      "blocks.9.ls2\n",
      "blocks.9.drop_path2\n",
      "blocks.10\n",
      "blocks.10.norm1\n",
      "blocks.10.attn\n",
      "blocks.10.attn.qkv\n",
      "blocks.10.attn.attn_drop\n",
      "blocks.10.attn.proj\n",
      "blocks.10.attn.proj_drop\n",
      "blocks.10.ls1\n",
      "blocks.10.drop_path1\n",
      "blocks.10.norm2\n",
      "blocks.10.mlp\n",
      "blocks.10.mlp.fc1\n",
      "blocks.10.mlp.act\n",
      "blocks.10.mlp.drop1\n",
      "blocks.10.mlp.fc2\n",
      "blocks.10.mlp.drop2\n",
      "blocks.10.ls2\n",
      "blocks.10.drop_path2\n",
      "blocks.11\n",
      "blocks.11.norm1\n",
      "blocks.11.attn\n",
      "blocks.11.attn.qkv\n",
      "blocks.11.attn.attn_drop\n",
      "blocks.11.attn.proj\n",
      "blocks.11.attn.proj_drop\n",
      "blocks.11.ls1\n",
      "blocks.11.drop_path1\n",
      "blocks.11.norm2\n",
      "blocks.11.mlp\n",
      "blocks.11.mlp.fc1\n",
      "blocks.11.mlp.act\n",
      "blocks.11.mlp.drop1\n",
      "blocks.11.mlp.fc2\n",
      "blocks.11.mlp.drop2\n",
      "blocks.11.ls2\n",
      "blocks.11.drop_path2\n",
      "norm\n",
      "fc_norm\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head): Linear(in_features=192, out_features=1000, bias=True)\n",
      ")\n",
      "PatchEmbed(\n",
      "  (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (norm): Identity()\n",
      ")\n",
      "Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "Identity()\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Sequential(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls1): Identity()\n",
      "    (drop_path1): Identity()\n",
      "    (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (drop1): Dropout(p=0.0, inplace=False)\n",
      "      (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "      (drop2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ls2): Identity()\n",
      "    (drop_path2): Identity()\n",
      "  )\n",
      ")\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "Block(\n",
      "  (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (attn): Attention(\n",
      "    (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls1): Identity()\n",
      "  (drop_path1): Identity()\n",
      "  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ls2): Identity()\n",
      "  (drop_path2): Identity()\n",
      ")\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=576, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=192, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Mlp(\n",
      "  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (drop1): Dropout(p=0.0, inplace=False)\n",
      "  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "  (drop2): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "Linear(in_features=192, out_features=768, bias=True)\n",
      "GELU(approximate='none')\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Linear(in_features=768, out_features=192, bias=True)\n",
      "Dropout(p=0.0, inplace=False)\n",
      "Identity()\n",
      "Identity()\n",
      "LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "Identity()\n",
      "Linear(in_features=192, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence only the blocks.0.attn.attn_drop with 0 varying until 11 is being extracted from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.attn_drop\n",
      "blocks.1.attn.attn_drop\n",
      "blocks.2.attn.attn_drop\n",
      "blocks.3.attn.attn_drop\n",
      "blocks.4.attn.attn_drop\n",
      "blocks.5.attn.attn_drop\n",
      "blocks.6.attn.attn_drop\n",
      "blocks.7.attn.attn_drop\n",
      "blocks.8.attn.attn_drop\n",
      "blocks.9.attn.attn_drop\n",
      "blocks.10.attn.attn_drop\n",
      "blocks.11.attn.attn_drop\n"
     ]
    }
   ],
   "source": [
    "attention_layer_name = 'attn_drop'\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if attention_layer_name in name:\n",
    "        print(name)\n",
    "        # module.register_forward_hook(self.get_attention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the register_forward_hook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT explanation:\n",
    "\n",
    "module.register_forward_hook: This method takes one argument, which is a hook function. When registered to a module (layer) in a PyTorch model, this hook function will be automatically called every time the forward method of the module has been executed.\n",
    "\n",
    "Purpose: By registering self.get_attention as a forward hook to the attention layers (identified by attention_layer_name in the model), the VITAttentionRollout class is designed to collect and store all the attention matrices produced during the forward pass of the input tensor through the model. This can be particularly useful for interpretability and visualization, helping to understand what the model is \"looking at\" or considering important in the input data.\n",
    "\n",
    "So, in summary, module.register_forward_hook(self.get_attention) allows your custom class VITAttentionRollout to automatically capture and record the outputs of the specified attention layers each time they process an input, without having to manually modify the model's forward method or its internal structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> So basically every time we do a forward pass on the model structure (like when we want to know the class of the image we are looking at), this function will also be called (like if it was added to the model structure itself). Therefore, most likely, the way we get the attention layers is by storing all of them when we pass through the model the image we want to classify **(does it change the weights?)** (interesting to verify, see the get_attention in VITARollout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of code for getting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select the rollout strategy to be used and display results\n",
    "#@markdown **Note** that for `grad_attention_rollout` passing a category index is mandatory.\n",
    "rollout_strategy = \"attention_rollout\" #@param [\"attention_rollout\", \"grad_attention_rollout\"]\n",
    "category_index =  264#@param {type:\"integer\"}\n",
    "\n",
    "print(f\"Using {rollout_strategy}\")\n",
    "\n",
    "input_tensor  = preprocess_image(\"examples/input.png\", transform) ## Gets the input tensor in the correct format\n",
    "\n",
    "## Not interesting ##\n",
    "if rollout_strategy == \"grad_attention_rollout\" and category_index < 0:\n",
    "    raise Exception(\"Category index is mandatory when using Gradient Attention Rollout\")\n",
    "\n",
    "elif rollout_strategy == \"grad_attention_rollout\" and category_index > 0:\n",
    "    grad_rollout = VITAttentionGradRollout(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = grad_rollout(input_tensor, category_index)\n",
    "    name = \"grad_rollout_{}_{:.3f}_{}.png\".format(category_index,\n",
    "        DISCARD_RATIO, \"mean\")\n",
    "## Not interesting ##\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "elif rollout_strategy == \"attention_rollout\":\n",
    "    attention_rollout = VITAttentionRollout(model, discard_ratio=DISCARD_RATIO) \n",
    "    # At this point, the input image was not yet passed thorugh the model\n",
    "    mask = attention_rollout(input_tensor) # here is the moment it passes, hence it might influence the attention layers (?)\n",
    "    # Here in mask we already have the final image..s...\n",
    "    name = \"attention_rollout_{:.3f}_{}.png\".format(DISCARD_RATIO, \"mean\")\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "## Only plotting until the end ##\n",
    "\n",
    "np_img = np.array(img)[:, :, ::-1]\n",
    "mask = cv2.resize(mask, (np_img.shape[1], np_img.shape[0])) #### since mask is 14 x 14, reshape to fit initial image size\n",
    "mask = show_mask_on_image(np_img, mask)\n",
    "\n",
    "# mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "# mask = mask.clip(0.7,1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(img)\n",
    "_ = ax2.imshow(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of attention layers include the image information since we look at the $V$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
